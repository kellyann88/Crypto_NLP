{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda update pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from datetime import datetime\n",
    "import json  \n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_item(text, start_marker=None, end_marker=None):\n",
    "    text = text.strip() # clean off \\n at the start and end of the text\n",
    "    \n",
    "    start_position = None\n",
    "    end_position = None\n",
    "    \n",
    "    if start_marker is not None:\n",
    "        start_position = text.find(start_marker) + len(start_marker)\n",
    "        \n",
    "\n",
    "    if end_marker is not None:\n",
    "        end_position = text.find(end_marker)\n",
    "        \n",
    "    extracted_item = text[start_position:end_position].strip()\n",
    "    return extracted_item\n",
    "\n",
    "base_path = \"texts\"\n",
    "filenames = glob.glob(os.path.join(base_path,\"*.txt\"))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_library = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        whole_text = f.read()\n",
    "\n",
    "    articles_list = whole_text.split('End of Document\\n\\n\\n')\n",
    "    useless_endmatter = articles_list.pop(-1) #.pop removes the item at the index specified and return it\n",
    "    \n",
    "    for text in articles_list:\n",
    "        title = extract_text_item(text, start_marker=None, end_marker='\\n')\n",
    "        body = extract_text_item(text, start_marker='Body\\n\\n\\n', end_marker='Load-Date:').strip()\n",
    "        article_data = {'Title': title, 'Body':body}\n",
    "        article_library.append(article_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(article_library)\n",
    "\n",
    "df = pd.DataFrame(article_library, columns=['Title', 'Body'])\n",
    "\n",
    "\n",
    "throwaway_df = df.copy()\n",
    "throwaway_df.index = throwaway_df ['Title']\n",
    "\n",
    "throwaway_df.index\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame()\n",
    "for f in glob.glob ('Excel_MetaData/*.xlsx'):\n",
    "    df2 = pd.read_excel(f)\n",
    "    all_data = all_data.append(df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('all_meta_data.xlsx', engine = 'xlsxwriter')\n",
    "all_data.to_excel(writer, sheet_name = 'Sheet1')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop_duplicates()\n",
    "print(df2)\n",
    "\n",
    "df2 = pd.read_excel('all_meta_data.xlsx', sheet_name = 'Sheet1')\n",
    "print (\"Column headings: \")\n",
    "print(df2.columns)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df, df2, on='Title', how='right')\n",
    "\n",
    "print (df3.columns)\n",
    "\n",
    "df3.drop(columns=['Unnamed: 0'], axis = 1)\n",
    "\n",
    "\n",
    "df3.drop_duplicates()\n",
    "\n",
    "\n",
    "df3.drop(columns=['Unnamed: 0'], axis = 1)\n",
    "\n",
    "writer = pd.ExcelWriter('df3.xlsx', engine = 'xlsxwriter')\n",
    "all_data.to_excel(writer, sheet_name = 'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "df3.info()\n",
    "\n",
    "df3.drop(columns = ['Show', 'Word count'])\n",
    "\n",
    "df3['Published date'] = pd.to_datetime(df3 ['Published date'], errors ='coerce')\n",
    "\n",
    "writer = pd.ExcelWriter('df3.xlsx', engine = 'xlsxwriter')\n",
    "all_data.to_excel(writer, sheet_name = 'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "df3.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_from_newsapi_2 = pd.read_json('newsapi_2a.json')\n",
    "\n",
    "df_from_newsapi_2 = pd.DataFrame(articles_from_newsapi_2)\n",
    "header=None\n",
    "\n",
    "df_from_newsapi_2\n",
    "\n",
    "df_from_newsapi_2['publishedAt'] = pd.to_datetime(df_from_newsapi_2['publishedAt'])\n",
    "\n",
    "df_from_newsapi_2\n",
    "\n",
    "df_from_newsapi_2.columns\n",
    "\n",
    "df_from_newsapi_2['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnesting(df, explode, axis):\n",
    "    if axis==1:\n",
    "        idx = df.index.repeat(df[explode['source']].str.len())\n",
    "        df1 = pd.concat([\n",
    "            pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)\n",
    "        df1.index = idx\n",
    "\n",
    "        return df1.join(df.drop(explode, 1), how='left')\n",
    "    else :\n",
    "        df1 = pd.concat([\n",
    "                         pd.DataFrame(df[x].tolist(), index=df.index).add_prefix(x) for x in explode], axis=1)\n",
    "        return df1.join(df.drop(explode, 1), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_newsapi_2 = unnesting(df_from_newsapi_2, ['source'], axis=0)\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"title\": \"Title\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.drop(columns = ['sourceid'])\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"content\": \"Body\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"author\": \"Author\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"sourcename\": \"Source\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"description\": \"Description\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"publishedAt\": \"Published Date\"})\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.drop(columns = ['urlToImage'])\n",
    "\n",
    "df_from_newsapi_2 = df_from_newsapi_2.rename(columns={\"url\": \"URL\"})\n",
    "\n",
    "df_from_newsapi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4 = concatenate df3 to df_from_newsapi_2\n",
    "frames = [df3, df_from_newsapi_2]\n",
    "result = pd.concat(frames) \n",
    "df4 = result\n",
    "axis = 1\n",
    "\n",
    "df4.Body.fillna(df4.Hlead, inplace=True)\n",
    "del df4['Hlead']\n",
    "\n",
    "df4 = df4.drop(columns = [\"Description\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Pub-copyright\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Publication type.1\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Publication.1\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Word count\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"URL\"])\n",
    "\n",
    "print(df4.columns)\n",
    "\n",
    "df4 = df4.drop(columns = [\"Ticker\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Show\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Section\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Term\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Headline\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Length\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Cite\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Agg-copyright\"])\n",
    "\n",
    "df4 = df4.drop(columns = [\"Publication type\"])\n",
    "\n",
    "print(df4.columns)\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter('df4.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "writer.save()\n",
    "\n",
    "df4 = df4.rename(columns={\"Published Date\": \"PublishedDate\"})\n",
    "\n",
    "df4 = df4.rename(columns={\"Published date\": \"Publisheddate\"})\n",
    "\n",
    "df4.head()\n",
    "\n",
    "print(df4.columns)\n",
    "\n",
    "writer = pd.ExcelWriter('df4.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "writer.save()\n",
    "\n",
    "cols = df4.columns.tolist()\n",
    "\n",
    "\n",
    "cols\n",
    "\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "cols\n",
    "\n",
    "df4 = df4[cols]\n",
    "\n",
    "\n",
    "df4.head()\n",
    "\n",
    "df4.info()\n",
    "\n",
    "writer = pd.ExcelWriter('df4.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"json\"\n",
    "jsonfilenames = glob.glob(os.path.join(base_path,\"*.json\"))\n",
    "jsonfilenames\n",
    "\n",
    "dfjson = pd.DataFrame()\n",
    "\n",
    "for file in jsonfilenames:\n",
    "        data = pd.read_json(file)    \n",
    "        data.append(data, ignore_index = True) \n",
    "    \n",
    "        dfjson = dfjson.append(data, ignore_index=True)\n",
    "\n",
    "dfjson.head()\n",
    "\n",
    "dfjson = unnesting(dfjson, ['source'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5 = concatenate df4 to dfjson\n",
    "frames = [df4, dfjson]\n",
    "result = pd.concat(frames) \n",
    "df5 = result\n",
    "axis = 1\n",
    "\n",
    "\n",
    "df5.head()\n",
    "\n",
    "df5.Author.fillna(df5.author, inplace=True)\n",
    "del df5['author']\n",
    "\n",
    "df5.Title.fillna(df5.title, inplace=True)\n",
    "del df5['title']\n",
    "\n",
    "df5.columns\n",
    "\n",
    "df5 = df5.drop(columns = ['url'])\n",
    "\n",
    "df5.Source.fillna(df5.sourceid, inplace=True)\n",
    "del df5['sourceid']\n",
    "\n",
    "df5.Source.fillna(df5.sourcename, inplace=True)\n",
    "del df5['sourcename']\n",
    "\n",
    "df5.Body.fillna(df5.content, inplace=True)\n",
    "del df5['content']\n",
    "\n",
    "df5 = df5.drop(columns = ['urlToImage'])\n",
    "\n",
    "df5.PublishedDate.fillna(df5.publishedAt, inplace=True)\n",
    "del df5['publishedAt']\n",
    "\n",
    "df5 = df5.drop(columns = [\"description\"])\n",
    "\n",
    "cols = df5.columns.tolist()\n",
    "\n",
    "cols\n",
    "\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "df5 = df5[cols]\n",
    "\n",
    "\n",
    "print(df5['Source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_from_newsapi_may = pd.read_json('df_may_newsapi.json')\n",
    "\n",
    "articles_from_newsapi_may = pd.DataFrame(articles_from_newsapi_may)\n",
    "header=None\n",
    "\n",
    "articles_from_newsapi_may.head()\n",
    "\n",
    "articles_from_newsapi_may.columns\n",
    "\n",
    "articles_from_newsapi_may['articles']\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.explode('articles')\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop(columns = [\"status\"])\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop(columns = [\"totalResults\"])\n",
    "\n",
    "articles_from_newsapi_may.head()\n",
    "\n",
    "list(articles_from_newsapi_may)\n",
    "\n",
    "articles_from_newsapi_may = unnesting(articles_from_newsapi_may, ['articles'], axis=0)\n",
    "\n",
    "articles_from_newsapi_may.head()\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop_duplicates(subset=['articlestitle'])\n",
    "\n",
    "articles_from_newsapi_may\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop(columns = [\"articlesdescription\"])\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop(columns = [\"articlesurl\"])\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.drop(columns = [\"articlesurlToImage\"])\n",
    "\n",
    "articles_from_newsapi_may.head()\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.rename(columns={\"articlesauthor\": \"Author\"})\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.rename(columns={\"articlespublishedAt\": \"PublishedDate\"})\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.rename(columns={\"articlescontent\": \"Body\"})\n",
    "\n",
    "articles_from_newsapi_may = articles_from_newsapi_may.rename(columns={\"articlestitle\": \"Title\"})\n",
    "\n",
    "articles_from_newsapi_may.head()\n",
    "\n",
    "articles_from_newsapi_may.reset_index(drop=True)\n",
    "\n",
    "articles_from_newsapi_may = unnesting(articles_from_newsapi_may, ['articlessource'], axis=0)\n",
    "\n",
    "articles_from_newsapi_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df6 = concatenate df5 to articles_from_newsapi_may\n",
    "frames1 = [df5, articles_from_newsapi_may]\n",
    "result1 = pd.concat(frames1) \n",
    "df6 = result1\n",
    "axis = 1\n",
    "\n",
    "df6.Source.fillna(df6.articlessourceid, inplace=True)\n",
    "\n",
    "\n",
    "df6.Source.fillna(df6.articlessourcename, inplace=True)\n",
    "del df6['articlessourcename']\n",
    "\n",
    "df6.columns\n",
    "\n",
    "df6.head()\n",
    "\n",
    "df6 = df6.drop_duplicates()\n",
    "\n",
    "df6 = df6.reset_index(drop=True)\n",
    "\n",
    "df6.Author.fillna(df6.Byline, inplace=True)\n",
    "del df6['Byline']\n",
    "\n",
    "df6.Author.fillna(df6.Source, inplace=True)\n",
    "\n",
    "\n",
    "df6.Author.fillna(df6.Company, inplace=True)\n",
    "\n",
    "\n",
    "df6.Source.fillna(df6.articlessourceid, inplace=True)\n",
    "del df6 ['articlessourceid']\n",
    "\n",
    "df6.Source.fillna(df6.Company, inplace=True)\n",
    "del df6 ['Company']\n",
    "\n",
    "df6.Source.fillna(df6.Publication, inplace=True)\n",
    "del df6 ['Publication']\n",
    "\n",
    "df6.Publisheddate.fillna(df6.PublishedDate, inplace=True)\n",
    "del df6['PublishedDate']\n",
    "\n",
    "df6 = df6.dropna(subset=['Body'], axis='index')\n",
    "\n",
    "\n",
    "df6.info()\n",
    "\n",
    "df6 = df6.set_index('Publisheddate', drop=True)\n",
    "\n",
    "df6.Author.fillna(df6.Source, inplace=True)\n",
    "\n",
    "\n",
    "df6[\"Publication location\"].fillna(\"International\", inplace = True)\n",
    "\n",
    "df6 = df6.where((pd.notnull(df6)), None)\n",
    "\n",
    "df6.info()\n",
    "\n",
    "\n",
    "writer1 = pd.ExcelWriter('df6.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "writer.save()\n",
    "\n",
    "\n",
    "df6.to_csv('df6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df6[\"Countries\"].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6['Source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df6[df6.Source.str.contains(\"Sophos.com\")].index\n",
    "df6.drop(to_drop , inplace=True)\n",
    "\n",
    "to_dropb = df6[df6.Source.str.contains(\"Thehackernews.com\")].index\n",
    "df6.drop(to_dropb , inplace=True)\n",
    "\n",
    "to_drop1 = df6[df6.Source.str.contains(\"Schneier.com\")].index\n",
    "df6.drop(to_drop1 , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"Huffpost.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "to_drop2 = df6[df6.Source.str.contains(\"Youtube.com\")].index\n",
    "df6.drop(to_drop2 , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"Bbc.com\", \"Countries\"] = \"United Kingdom\"\n",
    "\n",
    "df6.loc[df6.Source == \"bbc-news\", \"Countries\"] = \"United Kingdom\"\n",
    "\n",
    "to_drop3 = df6[df6.Source.str.contains(\"Techmeme.com\")].index\n",
    "df6.drop(to_drop3 , inplace=True)\n",
    "\n",
    "to_drop4 = df6[df6.Source.str.contains(\"engadget\")].index\n",
    "df6.drop(to_drop4 , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"Wipo.int\", \"Countries\"] = \"Switzerland\"\n",
    "\n",
    "df6.loc[df6.Source == \"Forbes.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"TechCrunch\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"techcrunch\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"ars-technica\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"the-next-web\", \"Countries\"] = \"Netherlands\"\n",
    "\n",
    "df6.loc[df6.Source == \"Yahoo.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"business-insider\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"Business Insider\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"Cnet.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"Technologyreview.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "to_drop5 = df6[df6.Source.str.contains(\"Gizmodo.com\")].index\n",
    "df6.drop(to_drop5 , inplace=True)\n",
    "\n",
    "to_drop6 = df6[df6.Source.str.contains(\"Gizmodo.com\")].index\n",
    "df6.drop(to_drop6 , inplace=True)\n",
    "\n",
    "to_drop7 = df6[df6.Source.str.contains(\"Koinpost.com\")].index\n",
    "df6.drop(to_drop7 , inplace=True)\n",
    "\n",
    "to_drop8 = df6[df6.Source.str.contains(\"Slashdot.org\")].index\n",
    "df6.drop(to_drop8 , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"Avclub\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"Avclub.com\", \"Countries\"] = \"United States\"\n",
    "\n",
    "to_drop9 = df6[df6.Source.str.contains(\"Lifehacker.com\")].index\n",
    "df6.drop(to_drop9 , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"Wired\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"wired\", \"Countries\"] = \"United States\"\n",
    "\n",
    "to_drop10 = df6[df6.Source.str.contains(\"Substack.com\")].index\n",
    "df6.drop(to_drop10 , inplace=True)\n",
    "\n",
    "to_drop10a = df6[df6.Source.str.contains(\"Fool.com\")].index\n",
    "df6.drop(to_drop10a , inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"the-new-york-times\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6.loc[df6.Source == \"al-jazeera-english\", \"Countries\"] = \"Qatar\"\n",
    "\n",
    "to_drop11 = df6[df6.Source.str.contains(\"Davidwalsh.name\")].index\n",
    "df6.drop(to_drop11 , inplace=True)\n",
    "\n",
    "to_drop11a = df6[df6.Source.str.contains(\"mashable\")].index\n",
    "df6.drop(to_drop11a , inplace=True)\n",
    "\n",
    "to_drop12 = df6[df6.Source.str.contains(\"Readwrite.com\")].index\n",
    "df6.drop(to_drop12 , inplace=True)\n",
    "\n",
    "to_drop12a = df6[df6.Source.str.contains(\"Androidcentral.com\")].index\n",
    "df6.drop(to_drop12a , inplace=True)\n",
    "\n",
    "to_drop13 = df6[df6.Source.str.contains(\"Marketwatch.com\")].index\n",
    "df6.drop(to_drop13 , inplace=True)\n",
    "\n",
    "to_drop13a = df6[df6.Source.str.contains(\"Venturebeat.com\")].index\n",
    "df6.drop(to_drop13a , inplace=True)\n",
    "\n",
    "to_drop14 = df6[df6.Source.str.contains(\"Boingboing.net\")].index\n",
    "df6.drop(to_drop14 , inplace=True)\n",
    "\n",
    "to_drop16 = df6[df6.Source.str.contains(\"Salon.com\")].index\n",
    "df6.drop(to_drop16 , inplace=True)\n",
    "\n",
    "to_drop16a = df6[df6.Source.str.contains(\"Theregister.co.uk\")].index\n",
    "df6.drop(to_drop16a , inplace=True)\n",
    "\n",
    "df6.drop(df6.index[df6.Source == \"Theregister\"], inplace=True)\n",
    "\n",
    "df6.drop(df6.index[df6.Source == \"Thehustle.co\"], inplace=True)\n",
    "\n",
    "df6.drop(df6.index[df6.Source == \"Androidauthority.com\"], inplace=True)\n",
    "\n",
    "df6.drop(df6.index[df6.Source == \"Entrepreneur.com\"], inplace=True)\n",
    "\n",
    "df6 = df6.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.loc[df6.Source == \"The New York Times - International Edition\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6['Source'].replace(['ALTRIA GROUP INC'], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace(['FACEBOOK INCPAYPAL HOLDINGS INC'], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"DUNKIN' BRANDS GROUP INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"WALL STREET JOURNALFACEBOOK INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"WALL STREET JOURNALFACEBOOK INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"UNIVERSITY OF TEXASUS DEPARTMENT OF JUSTICE\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"FACEBOOK INCVISA INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"FACEBOOK INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"FACEBOOK INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"FACEBOOK INCEUROPEAN CENTRAL BANK\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"DEMOCRATIC NATIONAL COMMITTEE\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"PAYPAL HOLDINGS INC\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6['Source'].replace([\"ContentEngine Think Tank Newswire English\"], \"libertarianinstitute.org\", inplace=True)\n",
    "\n",
    "df6.loc[df6.Source == \"libertarianinstitute.org\", \"Countries\"] = \"United States\"\n",
    "\n",
    "df6['Source'].replace([\"EASTMAN KODAK COLONG ISLAND ICED TEA CORP\"], \"The New York Times - International Edition\", inplace=True)\n",
    "\n",
    "df6.loc[df6['Countries'] == \"Foreign\"]\n",
    "\n",
    "df6['Countries'].replace([\"Foreign\"], \"United States\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.loc[df6['Source'] == \"vice-news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
