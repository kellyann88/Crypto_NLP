{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# DEFINE SETTINGS\n",
    "\n",
    "SAMPLE_SIZE = 800\n",
    "NUMBER_OF_CORES = 1\n",
    "PROCESSED_DF_FILENAME = 'my_processing.pkl'\n",
    "PHRASER_FILENAME = 'phraser.pkl'\n",
    "REPORT_EVERY_N = 10\n",
    "\n",
    "# DEFINE FUNCTIONS\n",
    "\n",
    "def filter_text(spacy_doc, phraser, stopwords):\n",
    "    transformed_doc = []\n",
    "    for sentence in spacy_doc.sents:\n",
    "        sentence_tokens = [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        transformed = phraser[sentence_tokens]\n",
    "        transformed_doc.extend(transformed)\n",
    "    tokens = [token for token in transformed_doc if token.lower() not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def train_phraser(spacy_generator, stopwords):\n",
    "    sentences = [\n",
    "        [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        for doc in spacy_generator \n",
    "        for sentence in doc.sents]\n",
    "    \n",
    "    bigram_phraser = gensim.models.Phrases(sentences, common_terms=stopwords)\n",
    "    return bigram_phraser\n",
    "\n",
    "# LOAD SPACY\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# LOAD IN DATA - CHECKING FOR PICKLE\n",
    "\n",
    "if os.path.exists(PROCESSED_DF_FILENAME):\n",
    "    df7 = pd.read_pickle(PROCESSED_DF_FILENAME)\n",
    "else:\n",
    "    filename = 'df6.csv'\n",
    "    df7 = pd.read_csv(filename)\n",
    "    df7['cleaned'] = \"\"\n",
    "\n",
    "# LOAD IN OR TRAIN AND SAVE PHRASER\n",
    "\n",
    "if os.path.exists(PHRASER_FILENAME):\n",
    "    with open(PHRASER_FILENAME,'rb') as f:\n",
    "        phraser = pickle.load(f)\n",
    "else:\n",
    "    print('Training Phraser')\n",
    "    docs = nlp.pipe(df7['Body'], n_process=NUMBER_OF_CORES)\n",
    "    phraser = train_phraser(docs, stopwords)\n",
    "    with open(PHRASER_FILENAME,'wb') as f:\n",
    "        pickle.dump(phraser,f)\n",
    "\n",
    "# SAMPLE THE DATASET\n",
    "\n",
    "to_process_filter = df7['cleaned'].apply(len) == 0\n",
    "todays_sample = df7[to_process_filter].head(SAMPLE_SIZE)\n",
    "\n",
    "# PROCESS THE SAMPLE\n",
    "\n",
    "docs = nlp.pipe(todays_sample['Body'], n_process=NUMBER_OF_CORES)\n",
    "cleaned_docs = []\n",
    "for i, spacy_doc in enumerate(docs, start=1):\n",
    "    if i % REPORT_EVERY_N == 0:\n",
    "        print(i, \"documents processed\")\n",
    "    cleaned = filter_text(spacy_doc, phraser, stopwords)\n",
    "    cleaned_docs.append(cleaned)\n",
    "\n",
    "# INSERT PROCESSED DOCS INTO DATAFRAME\n",
    "\n",
    "check_filter = df7['cleaned'].apply(len) != 0\n",
    "df7.loc[todays_sample.index,'cleaned'] = np.array(cleaned_docs, dtype='object')\n",
    "number_of_processed_rows = len(df7[df7['cleaned'].apply(len) !=0])\n",
    "\n",
    "# SAVE TO DISK\n",
    "\n",
    "df7.to_pickle(PROCESSED_DF_FILENAME)\n",
    "print(f\"Processed total: {number_of_processed_rows}/{len(df7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
