{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pickle\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df = pd.read_pickle('/Users/kellycoulter/Desktop/PhD_Code_2021/final_merged_df.pkl')\n",
    "\n",
    "final_merged_df\n",
    "\n",
    "texts = final_merged_df['Cleaned Tokens']\n",
    "\n",
    "vocab = corpora.Dictionary(texts) # id2word\n",
    "corpus = [vocab.doc2bow(s) for s in texts] #topic probability distribution shape-convert tokenized docs to vectors\n",
    "#the function doc2bow counts no of occurences of each distinct word, converts the word to its integer word id and returns result as sparse vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up and run LDA model over 20 topics and calculate coherence score-store in dictionary\n",
    "min_k = 2\n",
    "max_k = 21\n",
    "intervals = 1\n",
    "coherence = {}\n",
    "\n",
    "for i in range(min_k, max_k, intervals):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "    id2word=vocab,\n",
    "    num_topics=i,\n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    per_word_topics=True)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=vocab) \n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence[i] = coherence_lda\n",
    "    print('Coherence Score for {} Topics:'.format(i), coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot coherence scores on graph\n",
    "sm = pd.DataFrame.from_dict(coherence, orient='index', columns=['Coherence'])\n",
    "sm['Topics'] = sm.index\n",
    "\n",
    "plt.plot(sm.Topics, sm.Coherence)\n",
    "ticks = plt.xticks(np.arange(min(sm.Topics), max(sm.Topics)+1, 1))\n",
    "plt.savefig('img/coherence.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide range of alphas\n",
    "alphas = list(np.arange(0.01, 1, 0.3))\n",
    "print('alpha values: ', alphas)\n",
    "\n",
    "#provide range of betas\n",
    "betas = list(np.arange(0.01, 1, 0.3))\n",
    "print('beta values: ', alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected 14 topics as highest coherence score, now run lda model passing in default alpha and beta values calculating coherence\n",
    "n_topics_14 = 14\n",
    "evaluations_14 = []\n",
    "for alpha in alphas:\n",
    "    for beta in betas:\n",
    "        lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "            id2word=vocab,\n",
    "            num_topics=n_topics_14,\n",
    "            random_state=100,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha=alpha,\n",
    "            eta=beta)\n",
    "        coherence = CoherenceModel(model=lda, texts=texts, dictionary=vocab, coherence='c_v')\n",
    "        scores = [alpha, beta, coherence.get_coherence()]\n",
    "        evaluations_14.append(scores)\n",
    "        print(\"Finished a model, moving onto the next one...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of alpha, beta and coherence evaluations in 14 topic model\n",
    "evals_14 = pd.DataFrame(evaluations_14, columns = ['alpha', 'beta', 'coherence'])\n",
    "print(evals_14.sort_values('coherence', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot alpha and beta against coherence score in 14 topic model\n",
    "plt.style.use('classic')\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(xs=evals_14['alpha'],ys=evals_14['beta'],zs=evals_14['coherence'], marker='o', s=23, color='maroon')\n",
    "ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "ax.set_xlabel(r'$\\alpha$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\beta$', fontsize=20)\n",
    "ax.set_zlabel('Coherence', fontsize=12)\n",
    "plt.savefig('img/parameters14.pdf')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the highest alpha and beta scores run 14 topic model and show topics\n",
    "lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "id2word=vocab,\n",
    "num_topics=n_topics_14,\n",
    "random_state=100,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha=.91,\n",
    "eta=.91)\n",
    "for topic in lda.print_topics():\n",
    "    print('TOPIC {} | {}'.format(topic[0], topic[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using domain knowledge the above results display junk topics - topic 0, 10, 12, 13 junk topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next will try second highest coherence score which is 11 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected 11 topics as 2nd highest coherence score, now run lda model passing in default alpha and beta values calculating coherence\n",
    "n_topics_11 = 11\n",
    "evaluations_11 = []\n",
    "for alpha in alphas:\n",
    "    for beta in betas:\n",
    "        lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "            id2word=vocab,\n",
    "            num_topics=n_topics_11,\n",
    "            random_state=100,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha=alpha,\n",
    "            eta=beta)\n",
    "        coherence = CoherenceModel(model=lda, texts=texts, dictionary=vocab, coherence='c_v')\n",
    "        scores = [alpha, beta, coherence.get_coherence()]\n",
    "        evaluations_11.append(scores)\n",
    "        print(\"Finished a model, moving onto the next one...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of alpha, beta and coherence evaluations in 11 topic model\n",
    "evals_11 = pd.DataFrame(evaluations_11, columns = ['alpha', 'beta', 'coherence'])\n",
    "print(evals_11.sort_values('coherence', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot alpha and beta against coherence score in 11 topic model\n",
    "plt.style.use('classic')\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(xs=evals_11['alpha'],ys=evals_11['beta'],zs=evals_11['coherence'], marker='o', s=23, color='maroon')\n",
    "ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "ax.set_xlabel(r'$\\alpha$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\beta$', fontsize=20)\n",
    "ax.set_zlabel('Coherence', fontsize=12)\n",
    "plt.savefig('img/parameters11.pdf')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the highest alpha and beta scores run 11 topic model and show topics\n",
    "lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "id2word=vocab,\n",
    "num_topics=n_topics_11,\n",
    "random_state=100,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha=.91,\n",
    "eta=.91)\n",
    "for topic in lda.print_topics():\n",
    "    print('TOPIC {} | {}'.format(topic[0], topic[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next will try third highest coherence score which is 18 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected 18 topics as 3rd highest coherence score, now run lda model passing in default alpha and beta values calculating coherence\n",
    "n_topics_18 = 18\n",
    "evaluations_18 = []\n",
    "for alpha in alphas:\n",
    "    for beta in betas:\n",
    "        lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "            id2word=vocab,\n",
    "            num_topics=n_topics_18,\n",
    "            random_state=100,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha=alpha,\n",
    "            eta=beta)\n",
    "        coherence = CoherenceModel(model=lda, texts=texts, dictionary=vocab, coherence='c_v')\n",
    "        scores = [alpha, beta, coherence.get_coherence()]\n",
    "        evaluations_18.append(scores)\n",
    "        print(\"Finished a model, moving onto the next one...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of alpha, beta and coherence evaluations in 18 topic model\n",
    "evals_18 = pd.DataFrame(evaluations_18, columns = ['alpha', 'beta', 'coherence'])\n",
    "print(evals_18.sort_values('coherence', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot alpha and beta against coherence score in 18 topic model\n",
    "plt.style.use('classic')\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(xs=evals_18['alpha'],ys=evals_18['beta'],zs=evals_18['coherence'], marker='o', s=23, color='maroon')\n",
    "ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "ax.set_xlabel(r'$\\alpha$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\beta$', fontsize=20)\n",
    "ax.set_zlabel('Coherence', fontsize=12)\n",
    "plt.savefig('img/parameters18.pdf')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the highest alpha and beta scores run 18 topic model and show topics\n",
    "lda_18 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "id2word=vocab,\n",
    "num_topics=n_topics_18,\n",
    "random_state=100,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha=.91,\n",
    "eta=.91)\n",
    "for topic in lda.print_topics():\n",
    "    print('TOPIC {} | {}'.format(topic[0], topic[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "temp_lda_file = datapath('/Users/kellycoulter/Desktop/PhD_Code_2021/lda_18_model')\n",
    "\n",
    "lda.save(temp_lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=gensim.models.LdaMulticore.load(\"lda_18_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen 18 topic model-seems appropriate based on qualitative overview of related topics to crypto based on relevancy and representation\n",
    "#visualise 18 topic \n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, vocab, mds='tsne')\n",
    "pyLDAvis.save_html(vis, 'img/lda_18.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "lda_output = final_merged_df['Cleaned Tokens'].transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(final.n_components)]\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-benchmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-diamond",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
